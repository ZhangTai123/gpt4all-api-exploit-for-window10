import os
import sys

import fire
import gradio as gr
import torch
import transformers
from peft import PeftModel
from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer

from utils.callbacks import Iteratorize, Stream
from utils.prompter import Prompter

import json
from flask import Flask, render_template, request, session
# from flask import Flask
app = Flask(__name__)

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

try:
    if torch.backends.mps.is_available():
        device = "mps"
except:  # noqa: E722
    pass
def parameter():
    return
#def main(
#    load_8bit: bool = False,
#    base_model: str = "../../llama-7b-hf",
#    lora_weights: str = "../alpaca-lora-7b",
#    prompt_template: str = "",  # The prompt template to use, will default to alpaca.
#    server_name: str = "0.0.0.0",  # Allows to listen on all interfaces by providing '0.
#    share_gradio: bool = False,
#):
def evaluate(
    instruction,input,max_new_tokens
    #load_8bit: bool = False,
    #base_model: str = "../../llama-7b-hf",
    #lora_weights: str = "../alpaca-lora-7b",
    #prompt_template: str = "",  # The prompt template to use, will default to alpaca.
    #server_name: str = "0.0.0.0",  # Allows to listen on all interfaces by providing '0.
    #share_gradio: bool = False,
):
    #base_model = base_model or os.environ.get("BASE_MODEL", "")
    base_model = "../../llama-7b-hf"
    lora_weights = "../alpaca-lora-7b"
    load_8bit = True
    assert (
        base_model
    ), "Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'"
    prompt_template = ""  # The prompt template to use, will default to alpaca.
    prompter = Prompter(prompt_template)
    tokenizer = LlamaTokenizer.from_pretrained(base_model)
    if device == "cuda":
        model = LlamaForCausalLM.from_pretrained(
            base_model,
            load_in_8bit=load_8bit,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            torch_dtype=torch.float16,
        )
    elif device == "mps":
        model = LlamaForCausalLM.from_pretrained(
            base_model,
            device_map={"": device},
            torch_dtype=torch.float16,
        )
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            device_map={"": device},
            torch_dtype=torch.float16,
        )
    else:
        model = LlamaForCausalLM.from_pretrained(
            base_model, device_map={"": device}, low_cpu_mem_usage=True
        )
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            device_map={"": device},
        )

    # unwind broken decapoda-research config
    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk
    model.config.bos_token_id = 1
    model.config.eos_token_id = 2

    if not load_8bit:
        model.half()  # seems to fix bugs for some users.

    model.eval()
    if torch.__version__ >= "2" and sys.platform != "win32":
        model = torch.compile(model)
    prompt = prompter.generate_prompt(instruction,input)
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(device)
    generation_config = GenerationConfig(
        temperature=0.1,
        #temperature=temperature,
        top_p=0.75,
        top_k=40,
        num_beams=4,
    )

    generate_params = {
        "input_ids": input_ids,
        "generation_config": generation_config,
        "return_dict_in_generate": True,
        "output_scores": True,
        "max_new_tokens": max_new_tokens,
    }

    # Without streaming
    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_new_tokens,
        )
    s = generation_output.sequences[0]
    output = tokenizer.decode(s)
    #yield prompter.get_response(output)
    return prompter.get_response(output)

@app.route('/chat',methods=["POST","GET"])
def hello_flask():
    #Instruction_data = request.values.get('instruction')
    try:
        results = []
        Input_data = request.values.get('input')
        Instruction_data1 = "Rephrasing the input"
        Instruction_data2 = "Rewrite the input in a sad tone"
        Instruction_data3 = "Rewrite the input in a happy tone"
        if not Input_data :
            raise ValueError('Missing parameter')
        results1 = fire.Fire(evaluate(Instruction_data1,Input_data,128))
        results2 = fire.Fire(evaluate(Instruction_data2,Input_data,128))
        results3 = fire.Fire(evaluate(Instruction_data3,Input_data,128))
        results.append(results1)
        results.append(results2)
        results.append(results3)
        return json.dumps(results,ensure_ascii=False)
    except ValueError as e:
        # 处理异常情况
        error = {'error': str(e)}
        return jsonify(error), 400
    except Exception as e:
        # 记录异常信息
        app.logger.error(f'Unexpected error: {str(e)}')
        error = {'error': 'Internal server error'}
        return jsonify(error), 500


if __name__ == "__main__":
    #fire.Fire(main)
    #main
    app.run(host="0.0.0.0", port=7860, debug=False)

